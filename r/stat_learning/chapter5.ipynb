{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Resampling Methods  \n",
    "Once we have fitted a model using a set of observations we want to test its performance on new data. Three approaches are discussed in the book: validation set, cross validation and bootstrap. \n",
    "\n",
    "The validation set approach consists of splitting the available data set into two parts, the training set, used to fit our model, and the validation set, used to test the performance of our model. The problem of the validation set approach is that the model will have a lower performance than it could achieve if the full available data set could be used for training. \n",
    "\n",
    "Cross validation solves this problem by splitting the observations in k subsets, using one of them to test the model trained with the other k - 1 sets, repeating the procedure k times and taking the average error rate. The paramenter k can be small, 1, 5, 10 being the most used values, so that the problem raised by the validation set will not have impact. \n",
    "\n",
    "Bootstrap is a technique that can be used even with small data sets to estimate the accuracy of an estimate. it si similar cross-validation, but the bootstrap samples are created by taking random observations with replacement from the available observations.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.R Review Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>1000</li>\n",
       "\t<li>3</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 1000\n",
       "\\item 3\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 1000\n",
       "2. 3\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 1000    3"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>'X1'</li>\n",
       "\t<li>'X2'</li>\n",
       "\t<li>'y'</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 'X1'\n",
       "\\item 'X2'\n",
       "\\item 'y'\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 'X1'\n",
       "2. 'X2'\n",
       "3. 'y'\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] \"X1\" \"X2\" \"y\" "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load(\"data/5.R.RData\")\n",
    "dim(Xy); names(Xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard error for $\\beta_1$ is 0.02593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Call:\n",
       "lm(formula = y ~ X1 + X2, data = Xy)\n",
       "\n",
       "Residuals:\n",
       "     Min       1Q   Median       3Q      Max \n",
       "-1.44171 -0.25468 -0.01736  0.33081  1.45860 \n",
       "\n",
       "Coefficients:\n",
       "            Estimate Std. Error t value Pr(>|t|)    \n",
       "(Intercept)  0.26583    0.01988  13.372  < 2e-16 ***\n",
       "X1           0.14533    0.02593   5.604 2.71e-08 ***\n",
       "X2           0.31337    0.02923  10.722  < 2e-16 ***\n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
       "\n",
       "Residual standard error: 0.5451 on 997 degrees of freedom\n",
       "Multiple R-squared:  0.1171,\tAdjusted R-squared:  0.1154 \n",
       "F-statistic: 66.14 on 2 and 997 DF,  p-value: < 2.2e-16\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fit <- lm(y ~ X1 + X2, data = Xy)\n",
    "summary(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta1.fn <- function(data, index) {\n",
    "    X1 <- data$X1[index]\n",
    "    X2 <- data$X2[index]\n",
    "    fit <- lm(y ~ X1 + X2, data = data) \n",
    "    coef(fit)[2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Xy, statistic = beta1.fn, R = 100)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "     original  bias    std. error\n",
       "t1* 0.1453263       0           0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(boot)\n",
    "boot(Xy, beta1.fn, R = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.1 The Validation Set Approach\n",
    "The validation set approach is used to estimate the mean squared error (MSE)\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "where $y_i$ is an observation and $\\hat{f}(x_i)$ is a prediction from the fit. The set of available observation is split into two subsets. One subset is used for training and the other subset, the validation set, is used to test the quality of the fit by computing the MSE on unseen data. In the equation n is the size of the validation set. The observations in each subset are taken randomly from the original set of observations. In this example we use the Auto data set from the ISLR package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "      mpg          cylinders      displacement     horsepower      weight    \n",
       " Min.   : 9.00   Min.   :3.000   Min.   : 68.0   150    : 22   Min.   :1613  \n",
       " 1st Qu.:17.50   1st Qu.:4.000   1st Qu.:104.0   90     : 20   1st Qu.:2223  \n",
       " Median :23.00   Median :4.000   Median :146.0   88     : 19   Median :2800  \n",
       " Mean   :23.52   Mean   :5.458   Mean   :193.5   110    : 18   Mean   :2970  \n",
       " 3rd Qu.:29.00   3rd Qu.:8.000   3rd Qu.:262.0   100    : 17   3rd Qu.:3609  \n",
       " Max.   :46.60   Max.   :8.000   Max.   :455.0   75     : 14   Max.   :5140  \n",
       "                                                 (Other):287                 \n",
       "  acceleration        year           origin                  name    \n",
       " Min.   : 8.00   Min.   :70.00   Min.   :1.000   ford pinto    :  6  \n",
       " 1st Qu.:13.80   1st Qu.:73.00   1st Qu.:1.000   amc matador   :  5  \n",
       " Median :15.50   Median :76.00   Median :1.000   ford maverick :  5  \n",
       " Mean   :15.56   Mean   :75.99   Mean   :1.574   toyota corolla:  5  \n",
       " 3rd Qu.:17.10   3rd Qu.:79.00   3rd Qu.:2.000   amc gremlin   :  4  \n",
       " Max.   :24.80   Max.   :82.00   Max.   :3.000   amc hornet    :  4  \n",
       "                                                 (Other)       :368  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>397</li>\n",
       "\t<li>9</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 397\n",
       "\\item 9\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 397\n",
       "2. 9\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 397   9"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "library(ISLR)\n",
    "summary(Auto); dim(Auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a linear model using a random subset of observations, then we make predictions for the full set of observations but considering only the subset that have not been used to compute the MSE. The code below doesn't work with Jupyter but it does work in R and RStudio. Use the script chapter5.R in this same folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor horsepower has new levels 102, 107, 108, 132, 133, 138, 152, 190, 198, 200, 208, 210, 215, 220, 53, 54, 61, 64, 82, 91, 93\n",
     "output_type": "error",
     "traceback": [
      "Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor horsepower has new levels 102, 107, 108, 132, 133, 138, 152, 190, 198, 200, 208, 210, 215, 220, 53, 54, 61, 64, 82, 91, 93\nTraceback:\n",
      "1. predict(lm.fit, Auto.val)",
      "2. predict.lm(lm.fit, Auto.val)",
      "3. model.frame(Terms, newdata, na.action = na.action, xlev = object$xlevels)",
      "4. model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels)",
      "5. stop(sprintf(ngettext(length(m), \"factor %s has new level %s\", \n .     \"factor %s has new levels %s\"), nm, paste(nxl[m], collapse = \", \")), \n .     domain = NA)"
     ]
    }
   ],
   "source": [
    "set.seed(1)\n",
    "train <- sample(397, 196) # extract a random sample\n",
    "Auto.train <- Auto[train,]\n",
    "Auto.val <- Auto[-train,]\n",
    "lm.fit <- lm(mpg ~ horsepower, data= Auto, subset = train) # fit the model using the train data (it works in RStudio but not in Jupyter)\n",
    "lm.val <- predict(lm.fit, Auto.val) # using the model make predictions for the unseen observation\n",
    "mpg.val <- Auto$mpg[-train] # extract the values for mpg that have not been used for training\n",
    "square_residuals <- (mpg.val - lm.val)^2 # compute the square of the residuals for the validation set\n",
    "mse <- mean(square_residuals) # compute the mean squared error\n",
    "#mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3.4 The Bootstrap\n",
    "The bootstrap is a technique to obtain samples from a given data set by taking random samples with replacement of the original data set. As an example we have two stocks, X and Y, for which we have the returns and we want to build a portfolio allocating our money in such a way that the variance of our portfolio, that represents the risk, is minimized\n",
    "\n",
    "$$Var(\\alpha X + (1 - \\alpha) Y)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\alpha = \\frac{\\sigma_Y^2 - \\sigma_{XY}}{\\sigma_X^2 + \\sigma_Y^2 - 2 \\sigma_{XY}}$$\n",
    "\n",
    "In order to compute a good estimation of $\\sigma_X, \\sigma_Y$ and $\\sigma_{XY}$ and the accuracy of $\\alpha$ we can use the bootstrap to create many samples of the original data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       X                  Y           \n",
       " Min.   :-2.43276   Min.   :-2.72528  \n",
       " 1st Qu.:-0.88847   1st Qu.:-0.88572  \n",
       " Median :-0.26889   Median :-0.22871  \n",
       " Mean   :-0.07713   Mean   :-0.09694  \n",
       " 3rd Qu.: 0.55809   3rd Qu.: 0.80671  \n",
       " Max.   : 2.46034   Max.   : 2.56599  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>100</li>\n",
       "\t<li>2</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 100\n",
       "\\item 2\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 100\n",
       "2. 2\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 100   2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(Portfolio); dim(Portfolio)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the function $\\alpha$ as a function that takes as imput the data, the stock returns, and a randomized index for that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha.fn <- function(data, index) {\n",
    "    X <- data$X[index]\n",
    "    Y <- data$Y[index]\n",
    "    return ((var(Y) - cov(X, Y)) / (var(X) + var(Y) - 2 * cov(X,Y)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the boot() function from the 'boot' package to create random samples with replacement from the original data set and estimate the function $\\alpha$ and its standard error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "ORDINARY NONPARAMETRIC BOOTSTRAP\n",
       "\n",
       "\n",
       "Call:\n",
       "boot(data = Portfolio, statistic = alpha.fn, R = 1000)\n",
       "\n",
       "\n",
       "Bootstrap Statistics :\n",
       "     original       bias    std. error\n",
       "t1* 0.5758321 -0.001596422  0.09376093"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(1)\n",
    "boot(Portfolio, alpha.fn, R = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
