{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6 - Linear Model Selection and Regularization\n",
    "\n",
    "When we build a model we can use many predictors but we may not know in advance which predictor is more relevant for the predictions we are interested in. There are three approaches to select the models and the predictors: subset selection, shrinkage, also known as regularization, and dimension reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset selection\n",
    "Subset selection consists of sistematically trying all the predictors, adding or removing them and then comparing the accuracy of all of the resulting models, for example by comparing their residual sum of squares (RSS) on the validation set \n",
    "\n",
    "$$RSS = \\sum_{i=1}^{n}(y_i - \\hat{f}(x_i))^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\\hat{f}(x_1,..,x_p) = \\beta_0 + \\sum_{j=1}^{p} \\beta_j x_j$$\n",
    "\n",
    "or with other techniques such as the [Akaike Information Criterion](https://en.wikipedia.org/wiki/Akaike_information_criterion) that allow to compare the accuracy of models using all the available observations to train them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shrinkage (Regularization)\n",
    "The shrinkage method consists of putting a constrain on the values that can be bound to the parameters $\\beta_i$. Since the coefficients $\\beta_i$ in a linear model are estimated using the least squares method, by minimizing the residual sum of squares\n",
    "\n",
    "$$\\frac{\\partial RSS}{\\partial \\beta_i} = 0$$\n",
    "\n",
    "we can add a term to RSS that we can use to tune the values of $\\beta_i$ coefficients. The most common way is to add a term that is the sum of the squared coefficients so that, instead of RSS, the function to be minimized is\n",
    "\n",
    "$$RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$$\n",
    "\n",
    "We can then use the parameter $\\lambda$ to shrink the values of the coefficients towards zero. The technique of adding a term with squared coefficients to the expression to be minimized is called Ridge Regression or [Tikhonov Regularization](https://en.wikipedia.org/wiki/Regularization_(mathematics)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
