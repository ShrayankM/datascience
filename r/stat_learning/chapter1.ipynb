{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to Statistical Learning\n",
    "This is a collection of Jupyter notebooks from the course [Statistical Learning](https://courses.edx.org/courses/course-v1:StanfordOnline+STATSX0001+1T2020/course/) taught by Prof. Trevor Hastie and Prof. Robert Tibshirani, offered by StanfordOnline on the edX platform. The course is based on the book [An Introduction to Statistical Learning with Applications in R](http://faculty.marshall.usc.edu/gareth-james/ISL/code.html) by James Gareth et al.. The notebooks are based on the Lab exercises that are at the end of each chapter in the book. The programming language is R. Each notebook begins with a short summary of the topics discussed in the online course and in the related chapters in the book, and then proceeds with the exercises. I wrote these notebooks because I think that writing helps in clarifying the subject discussed. I hope that my summaries of the chapters still make sense and will help to remind me quickly the material learned. \n",
    "\n",
    "The course is mostly about **supervised learning**, in which we have a data set of observations $x_i$ with labels $y_i$ that we can use to fit our model \n",
    "\n",
    "$$y = f(x)$$\n",
    "\n",
    "in order to be able to figure out what can be the label of a new observation. The label, or response, can be a numerical value, such as in **regression problems** or a category such as in **classification problems**. The last chapter of the book provides an introduction to two **unsupervised learning** methods in which we have observations but no labels and our goal is to see whether there is some structure or pattern in the data. As can be seen from the chapters, half of the book is devoted to **linear models** in which we represent the relationship between the p predictors of the $i$th observation and the response by a linear function\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + ... + \\beta_p x_{ip}$$\n",
    "\n",
    "and our goal is to use the available observations to learn the values of the parameters $\\beta$.The 2nd part of the book presents different ways to overcome the limits of the linear models by adding higher order terms to the linear functions (polynomials and splines). The goal with models that include higher order terms will be to learn their parmeters in adition to the parameters of the linear terms \n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + \\beta_3 x_i^3 + ... + \\beta_d x_i^d$$\n",
    "\n",
    "A different approach is to use non-parametric methods by finding rules or similarities in the data without using mathematical models such as in **decision trees**, **splines** or **K-Means**. Support vector machines offer a geometrical approach mostly used in classification tasks. A theme that is transversal to all the techniques discussed in the course and in the book is **overfitting**, when our model performs well on the training data but not that much on new observations, and how to overcome it.\n",
    "\n",
    "\n",
    "2. [Statistical Learning](chapter2.ipynb)\n",
    "3. [Linear Regression](chapter3.ipynb)\n",
    "4. [Classification](chapter4.ipynb)\n",
    "5. [Resampling Methods](chapter5.ipynb)\n",
    "6. [Linear Model Selection and Regularization](chapter6.ipynb)\n",
    "7. [Moving Beyond Linearity](chapter7.ipynb)\n",
    "8. [Three-Based Methods](chapter8.ipynb)\n",
    "9. [Support Vector Machines](chapter9.ipynb)\n",
    "10. [Unsupervised Learning](chapter10.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
